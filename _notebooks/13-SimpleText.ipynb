{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Text\n",
    "\n",
    "In this example we go through a light example of processing a dataset for analyzing text!\n",
    "\n",
    "The data comes from this [website at Cornell](http://www.cs.cornell.edu/people/pabo/movie-review-data/) and is from Bo Pang and Lillian Lee, A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts, Proceedings of ACL 2004.\n",
    "\n",
    "This contains 1000 positive and 1000 negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes and Standard Magic...\n",
    "### Standard Magic and startup initializers.\n",
    "\n",
    "# Load Numpy\n",
    "import numpy as np\n",
    "# Load MatPlotLib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Load Pandas\n",
    "import pandas as pd\n",
    "# Load SQLITE\n",
    "import sqlite3\n",
    "# Load Stats\n",
    "from scipy import stats\n",
    "\n",
    "# This lets us show plots inline and also save PDF plots if we want them\n",
    "%matplotlib inline\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "matplotlib.style.use('fivethirtyeight')\n",
    "\n",
    "# These two things are for Pandas, it widens the notebook and lets us display data easily.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are both in huge directories so let's make 2 data frames.\n",
    "\n",
    "import glob\n",
    "\n",
    "all_pos = glob.glob(\"./data/review_polarity/pos/*\")\n",
    "all_neg = glob.glob(\"./data/review_polarity/neg/*\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a review and see what's up..\n",
    "with open('./data/review_polarity/pos/cv839_21467.txt') as f:\n",
    "    x = f.readlines()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a little messy so let's clean out some of the stuff and join it into one documet.\n",
    "import re\n",
    "re.sub(r'[.,\\'\\\"\\s\\s+]', \" \", \"\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we're read to read and fix up each of the reviews.\n",
    "\n",
    "revs = []\n",
    "\n",
    "for fname in all_pos:\n",
    "    rec = {}\n",
    "    with open(fname) as f:\n",
    "        x = f.readlines()\n",
    "    rec['text'] = re.sub(r'[.,\\'\\\"\\s\\s+]', \" \", \"\".join(x))\n",
    "    rec['sentiment'] = 'positive'\n",
    "    revs.append(rec)\n",
    "\n",
    "for fname in all_neg:\n",
    "    rec = {}\n",
    "    with open(fname) as f:\n",
    "        x = f.readlines()\n",
    "    rec['text'] = re.sub(r'[.,\\'\\\"\\s\\s+]', \" \", \"\".join(x))\n",
    "    rec['sentiment'] = 'negative'\n",
    "    revs.append(rec)\n",
    "    \n",
    "df_reviews = pd.DataFrame(revs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for the fun part...\n",
    "\n",
    "Now that we have some data to work with let's make some tf-idf vectors\n",
    "\n",
    "We're going to work with [tf-idf vectorizer from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "There are other options and a lot more you could do using sklearn! [See here](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the whole thing...\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize and play with token sizes...\n",
    "vec = TfidfVectorizer(min_df = 0.01, \n",
    "                      max_df = 0.99, \n",
    "                      ngram_range=(2,2)) # play with min_df and max_df\n",
    "\n",
    "# transform this into a sparse vector!\n",
    "vec.fit(df_reviews['text'])\n",
    "tf_idf_sparse = vec.transform(df_reviews[\"text\"])\n",
    "tf_idf_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're doing above is taking the reviews, and computing the tfidf scores for them if we cut off min_df and max_df, so we get letf with fewer words.  We can see the set of words along with the actual tfidf vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and for a review we can see the ROW of it's encoding.\n",
    "\n",
    "print(tf_idf_sparse[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now use this to classify the reviews!! but we need to test/train split again.\n",
    "\n",
    "# Split..\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_sparse, \n",
    "                                                    df_reviews['sentiment'], \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression(max_iter=100000, class_weight='balanced') \n",
    "model = logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always we should check our confusing matrix...\n",
    "# Check and confusion matrix...\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "plot_confusion_matrix(model, X_test, y_test,\n",
    "                                 display_labels=logisticRegr.classes_,\n",
    "                                 cmap=plt.cm.Blues, normalize='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.coef_[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also do cool stuff like make a dataframe with the words and see which\n",
    "# have the highest regression weights -- careful here!\n",
    "\n",
    "# Make a dataframe with the words, coefficients, and classes...\n",
    "recs = []\n",
    "for w,i in vec.vocabulary_.items():\n",
    "    recs.append([str(w)] + list(logisticRegr.coef_[:,i]))\n",
    "# If we only have one class then we only get weight..\n",
    "# df_weights = pd.DataFrame(tripples, columns=['word']+list(logisticRegr.classes_))\n",
    "df_weights = pd.DataFrame(recs, columns=['word', 'weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights.sort_values('weight', ascending=False)[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
