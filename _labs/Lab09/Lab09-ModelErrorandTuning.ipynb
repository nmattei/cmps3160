{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab09: Model Error and Tuning (5 Bonus Points)\n",
    "\n",
    "This lab is presented with some revisions from [Dennis Sun at Cal Poly](https://web.calpoly.edu/~dsun09/index.html) and his [Data301 Course](http://users.csc.calpoly.edu/~dsun09/data301/lectures.html)\n",
    "\n",
    "### When you have filled out all the questions, submit via [Tulane Canvas](https://tulane.instructure.com/)\n",
    "\n",
    "In the previous sections, we learned to build regression models. In this section, we will learn one way to evaluate the quality of a regression model: the training error. We will also discuss the shortcomings of using training error to measure the quality of a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 5\n",
    "\n",
    "housing = pd.read_csv(\"../data/ames.tsv\", sep=\"\\t\")\n",
    "housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics for Regression Models\n",
    "\n",
    "To evaluate the performance of a regression model, we compare the predicted labels from the model against the true labels. Since the labels are quantitative, it makes sense to look at the difference between each predicted label $\\hat y_i$ and the true label $y_i$. \n",
    "\n",
    "One way to make sense of these differences is to square each difference and average the squared differences. This measure of error is known as **mean squared error** (or **MSE**, for short):\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\textrm{MSE} &= \\textrm{mean of } (y_i - \\hat y_i)^2.\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "MSE is difficult to interpret because its units are the square of the units of $y$. To make MSE more interpretable, it is common to take the _square root_ of the MSE to obtain the **root mean squared error** (or RMSE, for short):\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\textrm{RMSE} &= \\sqrt{\\textrm{MSE}}.\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "The RMSE measures how off a \"typical\" prediction is. Notice that the reasoning above is exactly the same reasoning that we used in the early labs when we defined the variance and the standard deviation.\n",
    "\n",
    "Another common measure of error is the **mean absolute error** (or **MAE**, for short):\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\textrm{MAE} &= \\textrm{mean of } |y_i - \\hat y_i|.\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "Like the RMSE, the MAE measures how off a \"typical\" prediction is. There are other metrics that can be used to measure the quality of a regression model, but these are the most common ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Error\n",
    "\n",
    "To calculate the MSE, RMSE, or MAE, we need data where the true labels are known. Where do we find such data? One natural source of labeled data is the training data, since we needed the true labels to be able to train a model.\n",
    "\n",
    "For a $k$-nearest neighbors model, the training data is the data from which the $k$-nearest neighbors are selected. So to calculate the training RMSE, we do the following:\n",
    "\n",
    "For each observation in the training data:\n",
    "1. Find its $k$-nearest neighbors in the training data.\n",
    "2. Average the labels of the $k$-nearest neighbors to obtain the predicted label.\n",
    "3. Subtract the predicted label from the true label.\n",
    "\n",
    "At this point, we can average the square of these differences to obtain the MSE or average their absolute values to obtain the MAE.\n",
    "\n",
    "Let's calculate the training MSE for a 10-nearest neighbors model for house price using a subset of features from the Ames housing data set. First, we extract the variables that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features in our model. All quantitative, except Neighborhood.\n",
    "features = [\"Lot Area\", \"Gr Liv Area\",\n",
    "            \"Full Bath\", \"Half Bath\",\n",
    "            \"Bedroom AbvGr\", \n",
    "            \"Year Built\", \"Yr Sold\",\n",
    "            \"Neighborhood\"]\n",
    "\n",
    "X_train_dict = housing[features].to_dict(orient=\"records\")\n",
    "y_train = housing[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use Scikit-Learn to preprocess the features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(X_train_dict)\n",
    "X_train = vec.transform(X_train_dict)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_sc = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and to fit the $k$-nearest neighbors model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Fit a 10-nearest neighbors model.\n",
    "model = KNeighborsRegressor(n_neighbors=10)\n",
    "model.fit(X_train_sc, y_train)\n",
    "\n",
    "# Calculate the model predictions on the training data.\n",
    "y_train_pred = model.predict(X_train_sc)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to compare these predictions to the true labels, which we know, since this is the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean-squared error.\n",
    "mse = ((y_train - y_train_pred) ** 2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number is very large and not very interpretable (because it is in units of \"dollars squared\"). Let's take the square root to obtain the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE says that our model's predictions are, on average, off by about \\\\$33,000. This is not great, but it is also not too bad when an average house is worth about \\\\$180,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem with Training Error\n",
    "\n",
    "Training error is not a great measure of the quality of a model. To see why, consider a 1-nearest neighbor regression model. Before you read on, can you guess what the training error of a 1-nearest neighbor regression model will be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a 1-nearest neighbors model.\n",
    "model = KNeighborsRegressor(n_neighbors=1)\n",
    "model.fit(X_train_sc, y_train)\n",
    "\n",
    "# Calculate the model predictions on the training data.\n",
    "y_train_pred = model.predict(X_train_sc)\n",
    "\n",
    "# Calculate the MAE\n",
    "(y_train - y_train_pred).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error of this model seems too good to be true. Can our model really be off by just \\$57.85 on average?\n",
    "\n",
    "The error is so small because the nearest neighbor to any observation in the training data will be the observation itself! In fact, if we look at the vector of differences between the true and predicted labels, we see that most of the differences are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train - y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why isn't the MSE exactly equal to 0, then? That is because there may be multiple houses in the training data with the exact same values for all of the features, so there may be multiple observations that are a distance of 0.0 away. Any one of these observations has equal claim to being the \"1-nearest neighbor\". If we happen to select one of the _other_ houses in the training data as the nearest neighbor, then its price will in general be different.\n",
    "\n",
    "How many predictions did the 1-nearest neighbor model get wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train != y_train_pred).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1-nearest neighbor model nailed the price exactly for all but 22 of the 2930 houses, so the training error is small.\n",
    "\n",
    "Of course, a 1-nearest neighbor is unlikely to be the best model for predicting house prices. If one house in the training data happened to cost \\\\$10,000,000, it would not be sensible to predict another house to cost \\\\$10,000,000 -- even one very similar to it. This is why we usually average over multiple neighbors (i.e., $k$ neighbors) to make predictions.  \n",
    "\n",
    "In the next section, we will learn a better way to measure the quality of a model than training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** Using the Tips data set (`../data/tips.csv`), train $k$-nearest neighbors regression models to predict the tip for different values of $k$. Calculate the training MAE of each model and make a plot showing this training error as a function of $k$.\n",
    "\n",
    "What do you see? Is there an issue here?\n",
    "\n",
    "**Note that for this part you should only use the training MAE!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Answer Here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Test and Validation Errors\n",
    "\n",
    "In the previous section, we saw that training error is not a great measure of a model's quality. For example, a $1$-nearest neighbor model will have a training error of $0.0$ (or close to it), but it is not necessarily the best prediction model, especially if there are outliers in the training data.\n",
    "\n",
    "In order to come up with a better measure of model quality, we need to formalize what it is we want to measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 5\n",
    "\n",
    "housing = pd.read_csv(\"../data/ames.tsv\", sep=\"\\t\")\n",
    "housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Test Error\n",
    "\n",
    "Ultimately, the goal of any prediction model is to make predictions on _future_ data. Therein lies the problem with training error. Training error measures how well a model predicts on the current data. It is possible to build a model that **overfits** to the training data---that is, a model that fits so well to the current data that it does poorly on future data.\n",
    "\n",
    "For example, consider fitting two different models to the 10 training observations shown below. The model represented by the red line actually passes through every observation (that is, its training error is zero). However, most people would prefer the model represented by the blue line. If one had to make a prediction for $y$ when $x = 0.8$, the value of the blue line is intuitively more plausible than the value of the red line, which is out of step with the nearby points.\n",
    "\n",
    "![](../images/overfitting.png)\n",
    "\n",
    "The argument for the blue model depends on _future_ data because the blue model is actually worse than the red model on the current data. The red model tries so hard to get the predictions on the training data right that it ends up _overfitting_.\n",
    "\n",
    "If the goal is to build a model that performs well on future data, then we ought to evaluate it (i.e., by calculating MSE, MAE, etc.) on future data. The prediction error on future data is also known as **test error**, in contrast to training error, which is the prediction error on current data. To calculate the test error, we need _labeled_ future data. In many applications, future data is expensive to collect and _labeled_ future data is even more expensive. How can we approximate the test error, using just the data that we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Error\n",
    "\n",
    "The solution is to split the training data into a **training set** and a **validation set**. The model will only be fit on the observations of the training set. Then, the model will be evaluated on the validation set. Because the validation set has not been seen by the model already, it essentially plays the role of \"future\" data, even though it was carved out of the current data.\n",
    "\n",
    "The prediction error on the validation set is known as the **validation error**. The validation error is an approximation to the test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split our data into training and validation sets, we use the `.sample()` function in `pandas`. Let's use this to split our data into two equal halves, which we will call `train` and `val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = housing.sample(frac=.5)\n",
    "val = housing.drop(train.index)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this training/validation split to approximate the test error of a 10-nearest neighbors model.\n",
    "\n",
    "First, we extract the variables we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features in our model. All quantitative, except Neighborhood.\n",
    "features = [\"Lot Area\", \"Gr Liv Area\",\n",
    "            \"Full Bath\", \"Half Bath\",\n",
    "            \"Bedroom AbvGr\", \n",
    "            \"Year Built\", \"Yr Sold\",\n",
    "            \"Neighborhood\"]\n",
    "\n",
    "X_train_dict = train[features].to_dict(orient=\"records\")\n",
    "X_val_dict = val[features].to_dict(orient=\"records\")\n",
    "\n",
    "y_train = train[\"SalePrice\"]\n",
    "y_val = val[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use Scikit-Learn to preprocess the training and the validation data. Note that the vectorizer and the scaler are both fit to the training data, so we learn the categories, the mean, and standard deviation from the training set---and use these to transform both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# convert categorical variables to dummy variables\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(X_train_dict)\n",
    "X_train = vec.transform(X_train_dict)\n",
    "X_val = vec.transform(X_val_dict)\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "X_val_sc = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to fit a $k$-nearest neighbors model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Fit a 10-nearest neighbors model.\n",
    "model = KNeighborsRegressor(n_neighbors=10)\n",
    "model.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make predictions on the validation set and calculate the validation RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = model.predict(X_val_sc)\n",
    "rmse = np.sqrt(((y_val - y_val_pred) ** 2).mean())\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the test error is higher than the training error that we calculated in the previous section. In general, this will be true. It is harder for a model to predict for new observations it has not seen, than for observations it has seen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "One downside of the validation error above is that it was calculated using only 50% of the data. As a result, the estimate is noisy.\n",
    "\n",
    "There is a cheap way to obtain a second opinion of how well our model will do on future data. Previously, we split our data at random into two halves, training the model on the first half and evaluating it using the second half. Because the model has not already seen the second half of the data, this approximates how well the model would perform on future data. \n",
    "\n",
    "But the way we split our data was arbitrary. We might as well swap the roles of the two halves, training the model on the _second_ half and evaluating it using the _first_ half. As long as the model is always evaluated on data that is different from the data that was used to train it, we have a valid measure of how well our model would perform on future data. A schematic of this approach, known as **cross-validation**, is shown below.\n",
    "\n",
    "<img src=\"../images/cross-validation.png\" />\n",
    "\n",
    "Because we will be doing all computations twice, just with different data, let's wrap the $k$-nearest neighbors algorithm above into a function called `get_val_error()`, that computes the validation error given training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_error(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    # convert categorical variables to dummy variables\n",
    "    vec = DictVectorizer(sparse=False)\n",
    "    vec.fit(X_train_dict)\n",
    "    X_train = vec.transform(X_train_dict)\n",
    "    X_val = vec.transform(X_val_dict)\n",
    "\n",
    "    # standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_sc = scaler.transform(X_train)\n",
    "    X_val_sc = scaler.transform(X_val)\n",
    "    \n",
    "    # Fit a 10-nearest neighbors model.\n",
    "    model = KNeighborsRegressor(n_neighbors=10)\n",
    "    model.fit(X_train_sc, y_train)\n",
    "    \n",
    "    # Make predictions on the validation set.\n",
    "    y_val_pred = model.predict(X_val_sc)\n",
    "    rmse = np.sqrt(((y_val - y_val_pred) ** 2).mean())\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply this function to the training and test sets from earlier, we get the same estimate of the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_val_error(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we reverse the roles of the training and test sets, we get another estimate of the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_val_error(X_val, y_val, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two, somewhat independent estimates of the test error. It is common to average the two to obtain an overall estimate of the test error, called the **cross-validation error**. Notice that the cross-validation error uses each observation in the data exactly once. We make a prediction for each observation, but always using a model that was trained on data that does not include that observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** Use cross-validation to estimate the test error of a 1-nearest neighbor classifier on the Ames housing price data. How does a 1-nearest neighbor classifier compare to a 10-nearest neighbor classifier in terms of its ability to predict on _future_ data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.** Using the Tips data set (`../data/tips.csv`), train $k$-nearest neighbors regression models to predict the tip for different values of $k$. Calculate the training and validation MAE of each model, and make a plot showing these errors as a function of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Bringing It All Together: Model Selection and Hyperparameter Tuning\n",
    "\n",
    "This section will use the tools developed in the previous section to answer two important questions:\n",
    "\n",
    "- Model Selection: How do we determine which model is best?\n",
    "- Hyperparameter Tuning: How do we choose hyperparameters, such as $k$ in $k$-nearest neighbors?\n",
    "\n",
    "In the previous section, we saw how to use training and validation sets to estimate how well the model will perform on future data. A natural way to decide between competing models (or hyperparameters) is to choose the one that minimizes the validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 5\n",
    "\n",
    "housing = pd.read_csv(\"../data/ames.tsv\", sep=\"\\t\")\n",
    "housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $K$-Fold Cross Validation\n",
    "\n",
    "Previously, we carried out cross validation by splitting the data into 2 halves, alternately using one half to train the model and the other to evaluate the model. In general, we can split the data into $k$ subsamples, alternately training the data on $k-1$ subsamples and evaluating the model on the $1$ remaining subsample, i.e., the validation set. This produces $k$ somewhat independent estimates of the test error. This procedure is known as **$k$-fold cross validation**. (Be careful not to confuse the $k$ in $k$-fold cross validation with the $k$ in $k$-nearest neighbors.) Therefore, the specific version of cross validation that we saw earlier is $2$-fold cross validation.\n",
    "\n",
    "A schematic of $4$-fold cross validation is shown below.\n",
    "\n",
    "![](./images/k-folds.png)\n",
    "\n",
    "Implementing $k$-fold cross validation from scratch for $k > 2$ is straightforward but messy, so we will usually let Scikit-Learn do it for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation in Scikit-Learn\n",
    "\n",
    "Scikit-Learn provides a function, `cross_val_score`, that will carry out all aspects of $k$-fold cross validation: \n",
    "\n",
    "1. split the data into $k$ subsamples\n",
    "2. combine the first $k-1$ subsamples into a training set and train the model\n",
    "3. evaluate the model predictions on the last ($k$th) held-out subsample\n",
    "4. repeat steps 2-3 $k$ times (i.e. $k$ \"folds\"), each time holding out a different one of the $k$ subsamples\n",
    "4. calculate $k$ \"scores\", one from each validation set\n",
    "\n",
    "There is one subtlety to keep in mind. Training a $k$-nearest neighbors model is not just about fitting the model; it also involves dummifying the categorical variables and scaling the variables. These preprocessing steps should be included in the cross-validation process. They cannot be done ahead of time.\n",
    "\n",
    "For example, suppose we run $5$-fold cross validation. Then:\n",
    "\n",
    "- When subsamples 1-4 are used for training and subsample 5 for validation, the observations have to be standardized with respect to the mean and SD of subsamples 1-4.\n",
    "- When subsamples 2-5 are used for training and subsample 1 for validation, the observations have to be standardized with respect to the mean and SD of subsamples 2-5.\n",
    "- And so on.\n",
    "\n",
    "We cannot simply standardize all of the data once at the beginning and run cross validation on the standardized data. To do so would be allowing the model to peek at the validation set during training. That's because each training set would be standardized with respect to a mean and SD that is calculated from all data, including the validation set. To be completely above board, we should standardize each training set with respect to the mean and SD of just that training set.\n",
    "\n",
    "Fortunately, Scikit-Learn provides a `Pipeline` object that allows us to chain these preprocessing steps together with the model we want to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# get the features (in dict format) and the labels\n",
    "# (do not split into training and validation sets)\n",
    "features = [\"Lot Area\", \"Gr Liv Area\",\n",
    "            \"Full Bath\", \"Half Bath\",\n",
    "            \"Bedroom AbvGr\", \n",
    "            \"Year Built\", \"Yr Sold\",\n",
    "            \"Neighborhood\"]\n",
    "X_dict = housing[features].to_dict(orient=\"records\")\n",
    "y = housing[\"SalePrice\"]\n",
    "\n",
    "# specify the pipeline\n",
    "vec = DictVectorizer(sparse=False)\n",
    "scaler = StandardScaler()\n",
    "model = KNeighborsRegressor(n_neighbors=10)\n",
    "pipeline = Pipeline([(\"vectorizer\", vec), (\"scaler\", scaler), (\"fit\", model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This entire `Pipeline` can be passed to `cross_val_score`, along with the data, the number of folds $k$ (`cv`), and the type of score (`scoring`). So $5$-fold cross validation in Scikit-Learn would look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(pipeline, X_dict, y, \n",
    "                         cv=5, scoring=\"neg_mean_squared_error\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we get five (negative) validation MSEs, one from each of the 5 folds. `cross_val_score` returns the _negative_ MSE, instead of the MSE, because by definition, a _higher_ score is better. (Since we want the MSE to be as _low_ as possible, we want the negative MSE to be as _high_ as possible.)\n",
    "\n",
    "To come up with a single overall estimate of the test MSE, we flip the signs and average the MSEs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is the square root of the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "How do we choose $k$? We can simply try all values of $k$ and pick the one with the smallest (test) MSE.\n",
    "\n",
    "**Note:** Depending on your machine this may take a min to run.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer(sparse=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# calculates estimate of test error based on 10-fold cross validation\n",
    "def get_cv_error(k):\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    pipeline = Pipeline([(\"vectorizer\", vec), (\"scaler\", scaler), (\"fit\", model)])\n",
    "    mse = np.mean(-cross_val_score(\n",
    "        pipeline, X_dict, y, \n",
    "        cv=10, scoring=\"neg_mean_squared_error\"\n",
    "    ))\n",
    "    return mse\n",
    "    \n",
    "ks = pd.Series(range(1, 51))\n",
    "ks.index = range(1, 51)\n",
    "test_errs = ks.apply(get_cv_error)\n",
    "\n",
    "test_errs.plot.line()\n",
    "test_errs.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE is minimized near $k = 4$, which suggests that a $4$-nearest neighbors model is optimal for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Suppose we are not sure whether `Yr Sold` should be included in the $4$-nearest neighbors model or not. To determine whether or not it should be included, we can fit a model with `Yr Sold` included and another model with it excluded, and see which model has the better (test) MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer(sparse=False)\n",
    "scaler = StandardScaler()\n",
    "model = KNeighborsRegressor(n_neighbors=4)\n",
    "pipeline = Pipeline([(\"vectorizer\", vec), (\"scaler\", scaler), (\"fit\", model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Lot Area\", \"Gr Liv Area\",\n",
    "            \"Full Bath\", \"Half Bath\",\n",
    "            \"Bedroom AbvGr\", \n",
    "            \"Year Built\", \"Yr Sold\",\n",
    "            \"Neighborhood\"]\n",
    "X_dict = housing[features].to_dict(orient=\"records\")\n",
    "np.mean(\n",
    "    -cross_val_score(pipeline, X_dict, y, cv=10, scoring=\"neg_mean_squared_error\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Lot Area\", \"Gr Liv Area\",\n",
    "            \"Full Bath\", \"Half Bath\",\n",
    "            \"Bedroom AbvGr\", \n",
    "            \"Year Built\",\n",
    "            \"Neighborhood\"]\n",
    "X_dict = housing[features].to_dict(orient=\"records\")\n",
    "-cross_val_score(pipeline, X_dict, y, cv=10, scoring=\"neg_mean_squared_error\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE actually goes down when we remove `Yr Sold`, so it seems that the model is better off without this variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.** Using the Tips data set (`../data/tips.csv`), train $k$-nearest neighbors regression models to predict the tip for different values of $k$ and validate your model using 5 fold cross validation. Calculate the training and validation MAE of each model, and make a plot showing these errors as a function of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Question:** What is the best value for K?  Make sure to show a graph and value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS BONUS BONUS 5 Points:** Use what we've learned about model selection to determine whether or not including the day of the week is a good feature for a model to predict tips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Question:** Should we include day of the week?  Show a graph and values to show why or why not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you have filled out all the questions, submit via [Tulane Canvas](https://tulane.instructure.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
