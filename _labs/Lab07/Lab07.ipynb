{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Regression Models (5 Bonus Points Possible)\n",
    "\n",
    "This lab is presented with some revisions from [Dennis Sun at Cal Poly](https://web.calpoly.edu/~dsun09/index.html) and his [Data301 Course](http://users.csc.calpoly.edu/~dsun09/data301/lectures.html)\n",
    "\n",
    "### When you have filled out all the questions, submit via [Tulane Canvas](https://tulane.instructure.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, mount your google drive, change to the course folder, pull latest changes, and change to the lab folder.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/My Drive/cmps3160\n",
    "!git pull\n",
    "%cd _labs/Lab07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction problems are ubiquitous in real world applications. For example:\n",
    "\n",
    "- A real estate agent might want to predict the fair price of a home, using features of the home.\n",
    "- A sports bettor might want to predict which team will win the game, using information about the teams.\n",
    "- A historian might want to predict which historical figure wrote an anonymous document, using the words in the document.\n",
    "\n",
    "In each case, we have two kinds of variables:\n",
    "\n",
    "- **features** (a.k.a. **predictors**, **inputs**, **independent variables**), such as square footage and number of bedrooms, that are used to predict \n",
    "- a **label** (a.k.a. **response**, **output**, **dependent variable**), such as house price. \n",
    "\n",
    "We can formalize the problem mathematically as follows: let ${\\bf x}$ be the features and $y$ the label; a **predictive model** is a function $f$ that maps ${\\bf x}$ to $y$:\n",
    "\n",
    "$$ f: {\\bf x} \\mapsto y. $$\n",
    "\n",
    "Now suppose we have a new house, with features ${\\bf x}^*$. A predictive model $f$ predicts the price of this house to be $f({\\bf x}^*)$.\n",
    "\n",
    "How do we come up a predictive model $f$ in the first place? One way is to learn it from existing data, or **training data**. For example, to build a model that predicts the price of a home from the square footage (`Gr Liv Area`), we would need training data like the points shown in black below.\n",
    "\n",
    "<img src=\"../images/predictive_model.png\" />\n",
    "\n",
    "We could then learn a model, $f$, from this training data. For example, one possible predictive model is the red curve shown in the plot. This model was chosen to fit the points in the training data as tightly as possible. If we wanted to predict the price of a 2700 square foot home using this model, we would simply evaluate $f(2700)$, which comes out to about \\\\$300,000. The key thing to note is that $f$ depends on the training data. If the training data changes, then so does $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of learning predictive models from data is known as **machine learning**. There are many ways to learn a predictive model from data, including _linear regression_ (which you may have seen in a statistics course), _decision trees_, and _neural networks_. In this chapter, we will focus on one machine learning algorithm called **k-nearest neighbors** that leverages the distance metrics between observations we talked about in class.\n",
    "\n",
    "Predictive models are divided into two types, depending on whether the label $y$ is categorical or quantitative. If the label is quantitative, then the prediction problem is a **regression** problem, and the model is called a **regressor**. If the label is categorical, then the prediction problem is a **classification** problem, and the model is called a **classifier**.  We'll look at both of these models in the next couple of labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 K-Nearest Neighbors for Regression\n",
    "\n",
    "_Regressors_ are predictive models that are employed when the label is quantitative. In this section, we will train a machine learning model that predicts the price of a house from its square footage and other features.\n",
    "\n",
    "We will use the Ames housing data set as the training data. First, let's read in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_rows = 5\n",
    "\n",
    "housing = pd.read_csv(\"../data/ames.tsv\", sep=\"\\t\")\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on just two features for now: square footage (of the dwelling) and the number of bedrooms. Let's plot the training data, using a color gradient to represent the labels. Notice how we can customize the color gradient using the `cmap=` argument. A list of the available colormaps can be found [here](https://matplotlib.org/examples/color/colormaps_reference.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot.scatter(x=\"Gr Liv Area\", y=\"Bedroom AbvGr\", \n",
    "                     c=\"SalePrice\", cmap=\"plasma\", alpha=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how points that are close on this plot tend to have similar house prices. This insight is the basis of the $k$-nearest neighbors algorithm for predicting house prices. Suppose that we want to predict the price of a 4000 square foot home with 3 bedrooms, represented by a black circle on the plot below.\n",
    "\n",
    "<img src=\"../images/regression.png\" />\n",
    "\n",
    "We can find the $k$ points that are closest to this point in feature space and average their prices as our prediction. For example, the 30-nearest neighbors in the training data to the new home are illustrated in the plot below. We would average the prices of these 30 homes to obtain the predicted price for the new home.\n",
    "\n",
    "<img src=\"../images/regression_neighbors.png\" />\n",
    "\n",
    "The $k$-nearest neighbors regression algorithm can be summarized as follows:\n",
    "\n",
    "1. Determine the $k$ closest points in the training data to the new point that you want to predict for, based on some distance metric on the features.\n",
    "2. The predicted label of the new point is the mean (or median) of the labels of the $k$ closest points.\n",
    "\n",
    "Let's see how to implement this in code. First, we extract the training data and scale the features using z-scores as we discussed in class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = housing[[\"Gr Liv Area\", \"Bedroom AbvGr\"]]\n",
    "y_train = housing[\"SalePrice\"]\n",
    "\n",
    "X_train_mean = X_train.mean()\n",
    "X_train_std = X_train.std()\n",
    "X_train_sc = (X_train - X_train_mean) / X_train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a `Series` for the new house, scaling it in exactly the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = pd.Series(dtype=float)\n",
    "x_new[\"Gr Liv Area\"] = 4000\n",
    "x_new[\"Bedroom AbvGr\"] = 3\n",
    "\n",
    "x_new_sc = (x_new - X_train_mean) / X_train_std\n",
    "x_new_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the (Euclidean) distances between this new house and each house in the training data. Then, we sort the distances.\n",
    "\n",
    "Calculating the distance between two points is not as straightforward as it might seem because there is more than one way to define distance. The one most familiar to you is probably **Euclidan distance**, which is the straight-line distance (\"as the crow flies\") between the two points. The formula for calculating this distance is a generalization of the Pythagorean theorem:\n",
    "\n",
    "$$ d({\\bf x}, {\\bf x'}) = \\sqrt{\\sum_{j=1}^D (x_j - x'_j)^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = np.sqrt(((X_train_sc - x_new_sc) ** 2).sum(axis=1))\n",
    "dists_sorted = dists.sort_values()\n",
    "dists_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 30 entries of this sorted list are the 30 nearest neighbors. Let's get their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_nearest = dists_sorted.index[:30]\n",
    "i_nearest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look up these indices in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.loc[i_nearest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a prediction for the price of this new house, we average the sale prices of these 30 nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.loc[i_nearest].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model predicts that the house is worth $382,429."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Complex Model\n",
    "\n",
    "The model above only had two features so it was easy to visualize the \"nearest neighbors\" on the scatterplot. But the magic of $k$-nearest neighbors is that it still works when there are more features and the data isn't so easy to visualize.\n",
    "\n",
    "Let's create a model that has 8 features, some of which are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable\n",
    "housing[\"Date Sold\"] = housing[\"Yr Sold\"] + housing[\"Mo Sold\"] / 12\n",
    "features = [\"Lot Area\", \"Gr Liv Area\",\n",
    "            \"Full Bath\", \"Half Bath\",\n",
    "            \"Bedroom AbvGr\", \n",
    "            \"Year Built\", \"Date Sold\",\n",
    "            \"Neighborhood\"]\n",
    "\n",
    "# Note that \"Neighborhood\" is a categorical variable.\n",
    "X_train = pd.get_dummies(housing[features])\n",
    "y_train = housing[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose an assessor is trying to predict the fair value in 2011 of a 1400-square foot home built in 1980 with 3 bedrooms, 2 full baths, and 1 half bath, on a 9000 square-foot lot in the `OldTown` neighborhood. Let's create the `pandas` `Series` corresponding to this house. Remember that we have dummy variables for each neighborhood. We have to be sure to include these dummy variables in the new `Series` as well. The easiest way to do this is to initialize the index of the `Series` to match the columns of `X_train` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Series of NaNs, indexed by the columns of X_train\n",
    "x_new = pd.Series(index=X_train.columns, dtype=float)\n",
    "\n",
    "# Set the values of the known variables.\n",
    "x_new[\"Lot Area\"] = 9000\n",
    "x_new[\"Gr Liv Area\"] = 1400\n",
    "x_new[\"Full Bath\"] = 2\n",
    "x_new[\"Half Bath\"] = 1\n",
    "x_new[\"Bedroom AbvGr\"] = 3\n",
    "x_new[\"Year Built\"] = 1980\n",
    "x_new[\"Date Sold\"] = 2011\n",
    "\n",
    "# This house is in Old Town, so its dummy variable has value 1.\n",
    "x_new[\"Neighborhood_OldTown\"] = 1\n",
    "# The dummy variables for the other neighborhoods all have value 0.\n",
    "x_new.fillna(0, inplace=True)\n",
    "\n",
    "x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement $k$-nearest neighbors much as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the variables.\n",
    "X_train_mean = X_train.mean()\n",
    "X_train_std = X_train.std()\n",
    "\n",
    "X_train_sc = (X_train - X_train_mean) / X_train_std\n",
    "x_new_sc = (x_new - X_train_mean) / X_train_std\n",
    "\n",
    "# Find index of 30 nearest neighbors.\n",
    "dists = np.sqrt(((X_train_sc - x_new_sc) ** 2).sum(axis=1))\n",
    "i_nearest = dists.sort_values()[:30].index\n",
    "\n",
    "# Average the labels of these 30 nearest neighbors\n",
    "y_train.loc[i_nearest].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model predicts that this house is worth \\$132,343."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The K-Nearest Neighbors Regression Function\n",
    "\n",
    "Remember that a predictive model is a function $f: {\\bf x} \\mapsto y$. We can visualize $f$ when ${\\bf x}$ consists of a single feature, like square footage. We saw a hypothetical predictive model in Figure 5.1 above. What does $f$ look like when the model is a $k$-nearest neighbors regressor?\n",
    "\n",
    "First, we extract the training data. There is no need to scale the features in this case because there is only one feature. (The point of scaling is to bring all of the variables to the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = housing[[\"Gr Liv Area\"]]\n",
    "y_train = housing[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot $f$, we need to evaluate the predictive model at a grid of feature values. Since square footage varies from 0 to 6000 square feet in the training data, we create a grid of ${\\bf x}$ values from 0 to 6000, in increments of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pd.DataFrame()\n",
    "X_new[\"Gr Liv Area\"] = np.arange(0, 6000, 10)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a function `get_30NN_prediction` that implements the 30-nearest neighbor algorithm above: given a new observation, it returns the mean label of the 30-nearest neighbors to that observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_30NN_prediction(x_new):\n",
    "    \"\"\"Given new observation, returns 30-nearest neighbors prediction\n",
    "    \"\"\"\n",
    "    dists = np.sqrt((X_train - x_new) ** 2).sum(axis=1)\n",
    "    inds_sorted = dists.sort_values().index[:30]\n",
    "    return y_train.loc[inds_sorted].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually have 600 new observations in `X_new`. Let's apply this function to each new observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new_pred = X_new.apply(get_30NN_prediction, axis=1)\n",
    "y_new_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot these predictions as a curve (`.plot.line()`). `pandas` will plot the index of the `Series` on the `x`-axis, so we have to set the index appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new_pred.index = X_new[\"Gr Liv Area\"]\n",
    "y_new_pred.plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put all the pieces together and overlay this regression function on top of a scatterplot of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatterplot of the training data\n",
    "housing.plot.scatter(x=\"Gr Liv Area\", y=\"SalePrice\", color=\"black\", alpha=.2)\n",
    "\n",
    "# Add the predictions as a red line on this scatterplot\n",
    "y_new_pred.plot.line(color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how rough the 30-nearest neighbors regression function looks. In particular, look at the right half of the graph where the training data is sparse. The regression function is a step function in this range. That is because the value of the prediction changes only when the identities of the 30-nearest neighbors change. Houses with a square footage between 4500 and 6000 all have the same 30 nearest neighbors in the training data, so the prediction is constant in that range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** Plot the $k$-nearest neighbors regression function for predicting sale price from just its square footage for $k=5, 30, 100$. How does the regression function change as $k$ increases?\n",
    "\n",
    "**Hint:** To do this we'll need to modify the function above to take an argument being the number of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** You would like to predict how much a male diner will tip on a bill of \\\\$40.00 on a Sunday. Build a $k$-nearest neighbors model to answer this question, using the Tips dataset (`../data/tips.csv`) as your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE\n",
    "df_tips = pd.read_csv('../data/tips.csv')\n",
    "df_tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS BONUS BONUS Exercise (5 points).** We visualized the $k$-nearest neighbors regression function above, in the special case where there is only one feature. It is also possible to visualize a regression function in the case where there are two features, using a heat map, where the two axes represent the two features and the color represents the label.\n",
    "\n",
    "Make a heat map that shows the 30-nearest neighbors regression function when there are two features in the model: square footage (`Gr Liv Area`) and number of bedrooms (`Bedroom AbvGr`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: The Scikit-Learn API\n",
    "\n",
    "In the previous section, we implemented $k$-nearest neighbors from scratch. Now we will see how to implement it using [Scikit-Learn](http://scikit-learn.org/), a Python library that makes it easy to train and use machine learning models. All models are trained and used in the exact same way:\n",
    "\n",
    "1. Declare the model.\n",
    "2. Fit the model to training data, consisting of both features $X$ and labels $y$.\n",
    "3. Use the model to predict the labels for new values of the features.\n",
    "\n",
    "Let's take a look at how we would use this API to train a model on the Ames housing data set to predict the 2011 price of the Old Town house from the previous section. Scikit-Learn assumes that the data has already been completely converted to  quantitative variables and that the variables have already been standardized (if desired). The code below reads in the data and does the necessary preprocessing. \n",
    "\n",
    "(All of this code is copied from the previous section. Read the code, and if you are not sure what a particular line does, refer back to the previous section.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 5\n",
    "\n",
    "housing = pd.read_csv(\"./data/ames.tsv\", sep=\"\\t\")\n",
    "\n",
    "housing[\"Date Sold\"] = housing[\"Yr Sold\"] + housing[\"Mo Sold\"] / 12\n",
    "features = [\"Lot Area\", \"Gr Liv Area\",\n",
    "            \"Full Bath\", \"Half Bath\",\n",
    "            \"Bedroom AbvGr\", \n",
    "            \"Year Built\", \"Date Sold\",\n",
    "            \"Neighborhood\"]\n",
    "X_train = pd.get_dummies(housing[features])\n",
    "y_train = housing[\"SalePrice\"]\n",
    "\n",
    "x_new = pd.Series(index=X_train.columns, dtype=float)\n",
    "x_new[\"Lot Area\"] = 9000\n",
    "x_new[\"Gr Liv Area\"] = 1400\n",
    "x_new[\"Full Bath\"] = 2\n",
    "x_new[\"Half Bath\"] = 1\n",
    "x_new[\"Bedroom AbvGr\"] = 3\n",
    "x_new[\"Year Built\"] = 1980\n",
    "x_new[\"Date Sold\"] = 2011\n",
    "x_new[\"Neighborhood_OldTown\"] = 1\n",
    "x_new.fillna(0, inplace=True)\n",
    "\n",
    "X_train_mean = X_train.mean()\n",
    "X_train_std = X_train.std()\n",
    "X_train_sc = (X_train - X_train_mean) / X_train_std\n",
    "x_new_sc = (x_new - X_train_mean) / X_train_std\n",
    "\n",
    "X_train_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_train_sc` is a matrix of all numbers, which is the form that Scikit-Learn expects. Now let's see how to use Scikit-Learn to fit a $k$-nearest neighbors model to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Step 1: Declare the model.\n",
    "model = KNeighborsRegressor(n_neighbors=30)\n",
    "\n",
    "# Step 2: Fit the model to training data.\n",
    "model.fit(X_train_sc, y_train)\n",
    "\n",
    "# Step 3: Use the model to predict for new observations.\n",
    "# Scikit-Learn expects 2-dimensional arrays, so we need to \n",
    "# turn the Series into a DataFrame with 1 row.\n",
    "X_new_sc = x_new_sc.to_frame().T\n",
    "model.predict(X_new_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the exact same prediction that we got by implementing $k$-nearest neighbors manually (look back up to verify this is the case).\n",
    "\n",
    "In the case of training a machine learning model to predict for a single observation, Scikit-Learn may seem like overkill. In fact, the above Scikit-Learn code was 5 lines, whereas our implementation of $k$-nearest neighbors in the previous section was only 4 lines. However, learning Scikit-Learn will pay off as the problems become more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing in Scikit-Learn\n",
    "\n",
    "We constructed `X_train_sc` and `x_new_sc` above using just basic `pandas` operations. But it is also possible to have Scikit-Learn do this preprocessing for us. The preprocessing objects in Scikit-Learn all follow the same basic pattern:\n",
    "\n",
    "1. First, the preprocessing object has to be \"fit\" to a data set.\n",
    "2. The `.transform()` method actually processes the data. This method can be called repeatedly on multiple data sets and is guaranteed to process each data set in exactly the same way.\n",
    "\n",
    "It might not be obvious why it is necessary to first \"fit\" the preprocessing object to a data set before using it to process data. Hopefully, the following examples will make this clear.\n",
    "\n",
    "### Example 1: Dummy Encoding\n",
    "\n",
    "Instead of using `pd.get_dummies()`, we can do dummy encoding in Scikit-Learn using the `DictVectorizer` tool. There is one catch: `DictVectorizer` expects the data as a list of dictionaries, not as a `DataFrame`. But each row of a `DataFrame` can be represented as a dictionary, where the keys are the column names and the values are the data. `Pandas` provides a convenience function, `.to_dict()`, that converts a `DataFrame` into a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dict = housing[features].to_dict(orient=\"records\")\n",
    "X_train_dict[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pass this list to `DictVectorizer`, which will expand each categorical variable (e.g., \"Neighborhood\") into dummy variables. When the vectorizer is fit to the training data, it will learn all of the possible categories for each categorical variable so that when `.transform()` is called on different data sets, the same dummy variables will be returned (and in the same order). This is important for us because we need to apply the encoding to two data sets, the training data and the new observation, and we want to be sure that the same dummy variables appear in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(X_train_dict)\n",
    "\n",
    "X_train = vec.transform(X_train_dict)\n",
    "x_new_dict = {\n",
    "    \"Lot Area\": 9000,\n",
    "    \"Gr Liv Area\": 1400,\n",
    "    \"Full Bath\": 2,\n",
    "    \"Half Bath\": 1,\n",
    "    \"Bedroom AbvGr\": 3,\n",
    "    \"Year Built\": 1980,\n",
    "    \"Date Sold\": 2011,\n",
    "    \"Neighborhood\": \"OldTown\"\n",
    "}\n",
    "X_new = vec.transform([x_new_dict])\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Scaling\n",
    "\n",
    "We can also use Scikit-Learn to scale our data. The `StandardScaler` function standardizes data, but there are other functions, such as `Normalizer` and `MinMaxScaler`, that normalize and apply min-max scaling to the data, respectively. \n",
    "\n",
    "In the previous section, we standardized both the training data and the new observation with respect to the _training data_. To specify that the standardization should be with respect to the training data, we fit the scaler to the training data. Then, we use the scaler to transform both the training data and the new observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "X_new_sc = scaler.transform(X_new)\n",
    "\n",
    "X_train_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "The following example shows a complete pipeline: from reading in the raw data and processing it, to fitting a machine learning model and using it for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data.\n",
    "housing = pd.read_csv(\"../data/ames.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Define the features.\n",
    "housing[\"Date Sold\"] = housing[\"Yr Sold\"] + housing[\"Mo Sold\"] / 12\n",
    "features = [\"Lot Area\", \"Gr Liv Area\",\n",
    "            \"Full Bath\", \"Half Bath\",\n",
    "            \"Bedroom AbvGr\", \n",
    "            \"Year Built\", \"Date Sold\",\n",
    "            \"Neighborhood\"]\n",
    "\n",
    "# Define the training data.\n",
    "# Represent the features as a list of dicts.\n",
    "X_train_dict = housing[features].to_dict(orient=\"records\")\n",
    "X_new_dict = [{\n",
    "    \"Lot Area\": 9000,\n",
    "    \"Gr Liv Area\": 1400,\n",
    "    \"Full Bath\": 2,\n",
    "    \"Half Bath\": 1,\n",
    "    \"Bedroom AbvGr\": 3,\n",
    "    \"Year Built\": 1980,\n",
    "    \"Date Sold\": 2011,\n",
    "    \"Neighborhood\": \"OldTown\"\n",
    "}]\n",
    "y_train = housing[\"SalePrice\"]\n",
    "\n",
    "# Dummy encoding\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(X_train_dict)\n",
    "X_train = vec.transform(X_train_dict)\n",
    "X_new = vec.transform(X_new_dict)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "X_new_sc = scaler.transform(X_new)\n",
    "\n",
    "# K-Nearest Neighbors Model\n",
    "model = KNeighborsRegressor(n_neighbors=30)\n",
    "model.fit(X_train_sc, y_train)\n",
    "model.predict(X_new_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Part 2\n",
    "\n",
    "**Exercise 1.** (This exercise is identical to Exercise 2 from the previous section, except it asks you to use Scikit-Learn.) You would like to predict how much a male diner will tip on a bill of \\\\$40.00 on a Sunday. Use Scikit-Learn to build a $k$-nearest neighbors model to answer this question, using the Tips dataset (`./data/tips.csv`) as your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you have filled out all the questions, submit via [Tulane Canvas](https://tulane.instructure.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
