{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab10: Classification Models\n",
    "\n",
    "This lab is presented with some revisions from [Dennis Sun at Cal Poly](https://web.calpoly.edu/~dsun09/index.html) and his [Data301 Course](http://users.csc.calpoly.edu/~dsun09/data301/lectures.html)\n",
    "\n",
    "### When you have filled out all the questions, submit via [Tulane Canvas](https://tulane.instructure.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Classification models_ are used when the label we want to predict is categorical. In this section, we will train a classification model to predict the color of a wine (red or white) from its chemical properties. \n",
    "\n",
    "The training data for the red and white wines are stored in separate files on Github (`../data/reds.csv` and `../data/whites.csv`). Let's read in the two datasets, add a column for the color (\"red\" or \"white\"), and combine them into one `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 5\n",
    "\n",
    "reds = pd.read_csv(\"../data/reds.csv\", sep=\";\")\n",
    "whites = pd.read_csv(\"../data/whites.csv\", sep=\";\")\n",
    "\n",
    "reds[\"color\"] = \"red\"\n",
    "whites[\"color\"] = \"white\"\n",
    "\n",
    "wines = pd.concat([reds, whites], \n",
    "                  ignore_index=True)\n",
    "wines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on just two features for now: volatile acidity and total sulfur dioxide. Let's plot the training data, using color to represent the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = wines[\"color\"].map({\n",
    "    \"red\": \"darkred\",\n",
    "    \"white\": \"gold\"\n",
    "})\n",
    "\n",
    "wines.plot.scatter(\n",
    "    x=\"volatile acidity\", y=\"total sulfur dioxide\", c=colors, \n",
    "    alpha=.3, xlim=(0, 1.6), ylim=(0, 400)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we have a new wine with volatile acidity .85 and total sulfur dioxide 120, represented by a black circle in the plot below. Is this likely a red wine or a white wine?\n",
    "\n",
    "![](../images/classification.png)\n",
    "\n",
    "It is not hard to guess that this wine is probably red, just by looking at the plot. The reasoning goes like this: most of the wines in the training data that were \"close\" to this wine were red, so it makes sense to predict that this wine is also red. This is precisely the idea behind the $k$-nearest neighbors classifier:\n",
    "\n",
    "1. Calculate the distance between the new point and each point in the training data, using some distance metric on the features.\n",
    "2. Determine the $k$ closest points. Of these $k$ closest points, count up how many of each class label there are.\n",
    "3. The predicted class of the new point is whichever class was most common among the $k$ closest points.\n",
    "\n",
    "The only difference between the $k$-nearest neighbors classifier and the $k$-nearest neighbors regressor from the previous chapter is the last step. Instead of averaging the labels of the $k$ neighbors to obtain our prediction, we count up the number of occurrences of each category among the labels and take the most common one. It makes sense that we have to do something different because the label is now categorical instead of quantitative. **This is yet another example of the general principle that was introduced in Chapter 1: the analysis changes depending on the variable type!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the K-Nearest Neighbors Classifier\n",
    "\n",
    "Let's implement $9$-nearest neighbors for the wine above. First, we extract the training data and scale the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = wines[[\"volatile acidity\", \"total sulfur dioxide\"]]\n",
    "y_train = wines[\"color\"]\n",
    "\n",
    "X_train_sc = (X_train - X_train.mean()) / X_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a `Series` for the new wine, being sure to scale it in the exact same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = pd.Series(dtype=float)\n",
    "x_new[\"volatile acidity\"] = 0.85\n",
    "x_new[\"total sulfur dioxide\"] = 120\n",
    "\n",
    "x_new_sc = (x_new - X_train.mean()) / X_train.std()\n",
    "x_new_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the (Euclidean) distance between this new wine and each wine in the training data, and sort the distances from smallest to largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = np.sqrt(((X_train_sc - x_new_sc) ** 2).sum(axis=1))\n",
    "dists_sorted = dists.sort_values()\n",
    "dists_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 9 entries of this sorted list will be the 9 nearest neighbors. Let's get their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_nearest = dists_sorted.index[:9]\n",
    "inds_nearest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look up these indices in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines.loc[inds_nearest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, notice that these wines are all similar to the new wine in terms of volatile acidity and total sulfur dioxide. To make a prediction for this new wine, we need to count up how many reds and whites there are among these 9-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines.loc[inds_nearest, \"color\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were more reds than whites, so the 9-nearest neighbors model predicts that the wine is red.\n",
    "\n",
    "As a measure of confidence in a prediction, classification models often report the predicted _probability_ of each label, instead of just the predicted label. The predicted probability of a class in a $k$-nearest neighbors model is simply the proportion of the $k$ neighbors that are in that class. In the example above, instead of simply predicting that the wine is red, we could have instead said that the wine has a $6/9 = .667$ probability of being red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Classifier in Scikit-Learn\n",
    "\n",
    "Now let's see how to implement the same $9$-nearest neighbors model above using Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# define the training data\n",
    "X_train = wines[[\"volatile acidity\", \"total sulfur dioxide\"]]\n",
    "y_train = wines[\"color\"]\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "\n",
    "# fit the 9-nearest neighbors model\n",
    "model = KNeighborsClassifier(n_neighbors=9)\n",
    "model.fit(X_train_sc, y_train)\n",
    "\n",
    "# define the test data (Scikit-Learn expects a matrix)\n",
    "x_new = pd.DataFrame()\n",
    "x_new[\"volatile acidity\"] = [0.85]\n",
    "x_new[\"total sulfur dioxide\"] = [120]\n",
    "x_new_sc = scaler.transform(x_new)\n",
    "\n",
    "# use the model to predict on the test data\n",
    "model.predict(x_new_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want the predicted probabilities? For classification models, there is an additional method, `.predict_proba()`, that returns the predicted probability of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(x_new_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number represents the probability of the first class (\"red\") and the second number represents the probability of the second class (\"white\"). Notice that the predicted probabilities add up to 1, as they must."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** In the above example, we built a 9-nearest neighbors classifier to predict the color of a wine from just its volatile acidity and total sulfur dioxide. Use the model above to predict the color of a wine with the following features:\n",
    "\n",
    "- fixed acidity: 11\n",
    "- volatile acidity: 0.3\n",
    "- citric acid: 0.3\n",
    "- residual sugar: 2\n",
    "- chlorides: 0.08\n",
    "- free sulfur dioxide: 17\n",
    "- total sulfur dioxide: 60\n",
    "- density: 1.0\n",
    "- pH: 3.2\n",
    "- sulphates: 0.6\n",
    "- alcohol: 9.8\n",
    "- quality: 6\n",
    "\n",
    "Now, build a 15-nearest neighbors classifier using all of the features in the data set. Use this new model to predict the color of the same wine above.\n",
    "\n",
    "Does the predicted label change? Do the predicted probabilities of the labels change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Answer Here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** Build a 5-nearest neighbors model to predict whether or not a passenger on a Titanic would survive, based on their age, sex, and class as features. Use the Titanic data set (`../data/titanic.csv`) as your training data. Then, use your model to predict whether a 20-year old female in first-class would survive. What about a 20-year old female in third-class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Answer Here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Evaluating Classification Models\n",
    "\n",
    "Just as with regression models, we need ways to measure how good a classification model is. With regression models, the main metrics were MSE, RMSE, and MAE. With classification models, the main metrics are accuracy, precision, and recall. All of these metrics can be calculated on either the training data or the test data. We can also use cross validation to estimate the value of the metric on test data.\n",
    "\n",
    "First, let's train a $9$-nearest neighbors model on the wine data, just so that we have a model to evaluate. The following code is copied from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 5\n",
    "\n",
    "reds = pd.read_csv(\"../data/reds.csv\", sep=\";\")\n",
    "whites = pd.read_csv(\"../data/whites.csv\", sep=\";\")\n",
    "\n",
    "reds[\"color\"] = \"red\"\n",
    "whites[\"color\"] = \"white\"\n",
    "\n",
    "wines = pd.concat([reds, whites], \n",
    "                  ignore_index=True)\n",
    "wines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# define the training data\n",
    "X_train = wines[[\"volatile acidity\", \"total sulfur dioxide\"]]\n",
    "y_train = wines[\"color\"]\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "\n",
    "# fit the 9-nearest neighbors model\n",
    "model = KNeighborsClassifier(n_neighbors=9)\n",
    "model.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by calculating training metrics, so we need predictions for the observations in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train_sc)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Classification\n",
    "\n",
    "Because the labels $y_i$ in a classification model are categorical, we cannot calculate the difference $y_i - \\hat y_i$ between the actual and predicted labels, as we did for regression model. But we can determine if the predicted label $\\hat y_i$ is correct ($\\hat y_i = y_i$) or not ($\\hat y_i \\neq y_i$). For example, the **error rate** is defined to be:\n",
    "\n",
    "$$ \\textrm{error rate} = \\textrm{proportion where } \\hat y_i \\neq y_i $$\n",
    "\n",
    "With classification models, it is more common to report the performance in terms of a score, like **accuracy**, where a higher value is better:\n",
    "\n",
    "$$ \\textrm{accuracy} = \\textrm{proportion where } \\hat y_i = y_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (y_train_pred == y_train).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ever forget how to calculate accuracy, you can have Scikit-Learn do it for you. It just needs to know the true labels and the predicted labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with accuracy is that it is sensitive to the initial distribution of classes in the training data. For example, if 99% of the wines in the data set were white, it would be trivial to obtain a model with 99% accuracy: the model could simply predict that every wine is white. Even though such a model has high overall accuracy, it is remarkably bad for red wines. We need some way to measure the \"accuracy\" of a model for a particular class.\n",
    "\n",
    "Suppose we want to know the \"accuracy\" of our model for class $c$. There are two ways to interpret \"accuracy for class $c$\". Do we want to know the accuracy among the observations our model _predicted to be_ in class $c$ or the accuracy among the observations that _actually were_ in class $c$? The two options lead to two different notions of \"accuracy\" for class $c$: precision and recall.\n",
    "\n",
    "The **precision** of a model for class $c$ is the proportion of observations predicted to be in class $c$ that actually were in class $c$. \n",
    "\n",
    "$$ \\textrm{precision for class } c = \\frac{\\# \\{i:  \\hat y_i = c \\textrm{ and } y_i = c\\}}{\\# \\{i: \\hat y_i = c \\}}. $$\n",
    "\n",
    "The **recall** of a model for class $c$ is the proportion of observations actually in class $c$ that were predicted to be in class $c$.\n",
    "\n",
    "$$ \\textrm{recall for class } c = \\frac{\\# \\{i:  \\hat y_i = c \\textrm{ and } y_i = c\\}}{\\# \\{i: y_i = c \\}}. $$\n",
    "\n",
    "Another way to think about precision and recall is in terms of true positives (TP) and false positives (FP). A \"positive\" is an observation that the model identified as belonging to class $c$ (i.e., $\\hat y_i = c$). A true positive is one that actually was in class $c$ (i.e., $\\hat y_i = c$ and $y_i = c$), while a false positive is one that was not (i.e., $\\hat y_i = c$ and $y_i \\neq c$). True and false _negatives_ are defined analogously.\n",
    "\n",
    "In the language of positives and negatives, the precision is the proportion of positives that are true positives:\n",
    "$$ \\textrm{precision for class } c = \\frac{TP}{TP + FP}, $$\n",
    "while the recall is the proportion of observations in class $c$ that are positives (as opposed to negatives):\n",
    "$$ \\textrm{recall for class } c = \\frac{TP}{TP + FN}. $$\n",
    "\n",
    "The diagram below may help you to remember which numbers go in the numerator and denominator. The precision is the proportion of the red rectangle that is a TP, while the recall is the proportion of the red circle that is a TP.\n",
    "\n",
    "![](../images/precision_recall.png)\n",
    "\n",
    "Let's calculate the precision and recall of our $9$-nearest neighbors model for the red \"class\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = ((y_train_pred == \"red\") & (y_train == \"red\")).sum()\n",
    "\n",
    "precision = true_positives / (y_train_pred == \"red\").sum()\n",
    "recall = true_positives / (y_train == \"red\").sum()\n",
    "    \n",
    "precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can have Scikit-Learn calculate precision and recall for you. These functions work similarly to `accuracy_score` above, except we have to explicitly specify the class for which we want the precision and recall. For example, to calculate the precision and recall of the model for red wines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "(precision_score(y_train, y_train_pred, pos_label=\"red\"),\n",
    " recall_score(y_train, y_train_pred, pos_label=\"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to specify `pos_label` because the precision and recall for other classes may be quite different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(precision_score(y_train, y_train_pred, pos_label=\"white\"),\n",
    " recall_score(y_train, y_train_pred, pos_label=\"white\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there is a tradeoff between precision and recall. For example, you can improve recall by predicting more observations to be in class $c$, but this will hurt precision. To take an extreme example, a model that predicts that _every_ observation is in class $c$ has 100% recall, but its precision would likely be poor. To visualize this phenomenon, suppose we expand the positives from the dashed circle to the solid circle, as shown in the figure below, at right. This increases recall (because the circle now covers more of the red rectangle) but decreases precision (because the red rectangle now makes up a smaller fraction of the circle).\n",
    "\n",
    "![](../images/precision_recall_tradeoff.png)\n",
    "\n",
    "Likewise, you can improve precision by predicting fewer observations to be in class $c$ (i.e., only the ones you are very confident about), but this will hurt recall. This is illustrated in the figure above, at left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Accuracy, Precision, and Recall in Scikit-Learn\n",
    "\n",
    "We calculated the training accuracy of our classifier above. However, test accuracy is more useful in most cases. We can estimate the test accuracy using cross validation. We will have Scikit-Learn carry out the cross validation for us, including the computation of the accuracy score on each held-out subsample. We simply have to specify the right `scoring=` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", scaler),\n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "cross_val_score(pipeline, X_train, y_train, \n",
    "                cv=10, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a single estimate of test accuracy from the 10 validation accuracies, we can take their average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipeline, X_train, y_train, \n",
    "                cv=10, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy is still high, but lower than the training accuracy. This makes sense because it is always harder to predict for future data than for current data.\n",
    "\n",
    "Scikit-Learn can also calculate the precision and recall of a class $c$, but we need to manually convert the label to a binary label that is $1$ (or `True`) if the observation is in class $c$ and $0$ (or `False`) otherwise. For example, the following code calculates the validation _recall_ for red wines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_red_train = (y_train == \"red\")\n",
    "\n",
    "cross_val_score(pipeline, X_train, is_red_train, \n",
    "                cv=10, scoring=\"recall\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the validation _precision_ for red wines, we just have to change the scoring method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipeline, X_train, is_red_train, \n",
    "                cv=10, scoring=\"precision\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score: Combining Precision and Recall\n",
    "\n",
    "We have replaced accuracy by two numbers: precision and recall. We can combine the precision and recall into a single number, called the **F1 score**. \n",
    "\n",
    "The F1 score is defined to be the **harmonic mean** of the precision and the recall. That is, \n",
    "\n",
    "$$ \\frac{1}{\\text{F1 score}} = \\frac{ \\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}}{2}, $$\n",
    "\n",
    "or equivalently, \n",
    "\n",
    "$$ \\text{F1 score} = \\frac{2 \\cdot \\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}. $$\n",
    "\n",
    "The harmonic mean of two numbers is always between the two numbers, but in general will be closer to the smaller number. For example, if precision is $90\\%$ and recall is $10\\%$, then the harmonic mean is \n",
    "\n",
    "$$ \\text{F1 score} = \\frac{2 \\cdot 0.9 \\cdot 0.1}{0.9 + 0.1} = 18\\%. $$\n",
    "\n",
    "This is a desirable property of F1 scores because we want to encourage models to have both high precision _and_ high recall. It is not sufficient for one of these to be high if the other is very low. In other words, we do not want to allow a high precision to cancel out a low recall, or vice versa.\n",
    "\n",
    "The F1 score for red wines is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have also asked Scikit-Learn calculate this for us. If we know the actual and predicted labels, we can use the `f1_score` function, which works similarly to `precision_score` and `recall_score` from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_train, y_train_pred, pos_label=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have `cross_val_score` calculate and return the F1 scores on each held-out subsample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipeline, X_train, is_red_train, \n",
    "                cv=10, scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Exercises\n",
    "\n",
    "Exercises 3-5 ask you to use the Titanic data set (`../data/titanic.csv`) to train various classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.** Train a 5-nearest neighbors model to predict whether or not a passenger on a Titanic survived, using their age, sex, and class as features. Calculate the _training_ accuracy, precision, and recall of this model for survivors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.** Estimate the _test_ accuracy, precision, and recall of your model from Exercise 1 for survivors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.** You want to build a $k$-nearest neighbors model to predict where a Titanic passenger embarked, using their age, sex, and class. \n",
    "\n",
    "- What value of $k$ optimizes overall accuracy?\n",
    "- What value of $k$ optimizes the F1 score for Southampton?\n",
    "\n",
    "Does the same value of $k$ optimize accuracy and the F1 score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Answers**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you have filled out all the questions, submit via [Tulane Canvas](https://tulane.instructure.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
